{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HW 1  – Frequent Pattern Mining\n",
    "Name: Robb Alexander\n",
    "Class: CSCI349    \n",
    "Semester: 2021SP    \n",
    "Instructor: Brian King"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise\t1 – The\tapriori\talgorithm\n",
    "\n",
    "#### a) Find all frequent itemsets using the Apriori algorithm. Show your work.\n",
    "```\n",
    "min_sup = 60%\n",
    "min_conf = 80%\n",
    "\n",
    "1-itemsets:\n",
    "{M} 0.6\n",
    "{O} 0.6\n",
    "{N} 0.4 min_sup pruned\n",
    "{K} 1.0\n",
    "{E} 0.8\n",
    "{Y} 0.6\n",
    "{D} 0.2 min_sup pruned\n",
    "{A} 0.2 min_sup pruned\n",
    "{U} 0.2 min_sup pruned\n",
    "{C} 0.4 min_sup pruned\n",
    "{I} 0.2 min_sup pruned\n",
    "\n",
    "2-itemsets:\n",
    "{M, O} 0.2 min_sup pruned\n",
    "{M, N} --- apriori pruned\n",
    "{M, K} 0.6\n",
    "{M, E} 0.4 min_sup pruned\n",
    "{M, Y} 0.2 min_sup pruned\n",
    "{M, A} --- apriori pruned\n",
    "{M, U} --- apriori pruned\n",
    "{M, C} --- apriori pruned\n",
    "{O, N} --- apriori pruned\n",
    "{O, K} 0.6\n",
    "{O, E} 0.6\n",
    "{O, Y} 0.4 min_sup pruned\n",
    "{O, D} --- apriori pruned\n",
    "{O, C} --- apriori pruned\n",
    "{O, I} --- apriori pruned\n",
    "{N, *} --- apriori pruned\n",
    "{K, E} 0.8\n",
    "{K, Y} 0.6\n",
    "{K, A} --- apriori pruned\n",
    "{K, C} --- apriori pruned\n",
    "{K, I} --- apriori pruned\n",
    "{E, Y} 0.4 min_sup pruned\n",
    "{E, D} --- apriori pruned\n",
    "{E, A} --- apriori pruned\n",
    "{E, I} --- apriori pruned\n",
    "{E, C} --- apriori pruned\n",
    "{D, *} --- apriori pruned\n",
    "{A, *} --- apriori pruned\n",
    "{U, *} --- apriori pruned\n",
    "{C, *} --- apriori pruned\n",
    "{I, *} --- apriori pruned\n",
    "\n",
    "3-itemsets:\n",
    "{M, K, O} 0.2 min_sup pruned\n",
    "{M, K, E} 0.4 min_sup pruned\n",
    "{M, K, Y} 0.4 min_sup pruned\n",
    "{O, K, E} 0.6\n",
    "{O, K, Y} 0.4 min_sup pruned\n",
    "\n",
    "itemsets:\n",
    "{M} 0.6\n",
    "{O} 0.6\n",
    "{K} 1.0\n",
    "{E} 0.8\n",
    "{Y} 0.6\n",
    "{M, K} 0.6\n",
    "{O, K} 0.6\n",
    "{O, E} 0.6\n",
    "{K, E} 0.8\n",
    "{K, Y} 0.6\n",
    "{O, K, E} 0.6\n",
    "```\n",
    "\n",
    "#### b) What is a closed frequent itemset? List\n",
    "Closed frequent itemset: An itemset that is frequent but does not have the same support value as its superset.\n",
    "```\n",
    "{K}\n",
    "{M, K}\n",
    "{K, E}\n",
    "{K, Y}\n",
    "{O, K, E}\n",
    "```\n",
    "#### c) What is a max frequent itemset? List\n",
    "Closed frequent itemset: An itemset that is frequent but does not have any supersets above that is considered frequent.\n",
    "```\n",
    "{M, K}\n",
    "{K, Y}\n",
    "{O, K, E}\n",
    "```\n",
    "\n",
    "#### d) Generate all strong association rules\n",
    "```\n",
    "{M} -> {K}  confidence: 1.0  lift: 1.0\n",
    "{K} -> {M}  confidence: 0.6  lift: 1.0\n",
    "{K} -> {E}  confidence: 0.8  lift: 1.0\n",
    "{E} -> {K}  confidence: 1.0  lift: 1.0\n",
    "{O} -> {E}  confidence: 1.0  lift: 1.25\n",
    "{E} -> {O}  confidence: 0.75  lift: 1.25\n",
    "{O} -> {K}  confidence: 1.0  lift: 1.0\n",
    "{K} -> {O}  confidence: 0.6  lift: 1.0\n",
    "{K} -> {Y}  confidence: 0.6  lift: 1.0\n",
    "{Y} -> {K}  confidence: 1.0  lift: 1.0\n",
    "{O, K} -> {E}  confidence: 1.0  lift: 1.25\n",
    "{K, E} -> {O}  confidence: 0.75  lift: 1.25\n",
    "{O, E} -> {K}  confidence: 1.0  lift: 1.0\n",
    "{K} -> {O, E}  confidence: 0.6  lift: 1.0\n",
    "{O} -> {K, E}  confidence: 1.0  lift: 1.25\n",
    "{E} -> {K, O}  confidence: 0.75  lift: 1.25\n",
    "```\n",
    "#### e) What is the strongest rule output\n",
    "```\n",
    "{O, K} -> {E}  confidence: 1.0  lift: 1.25\n",
    "{O} -> {K, E}  confidence: 1.0  lift: 1.25\n",
    "```\n",
    "These are the strongest rules because they have the highest confidence and also the most dependency due to the highest lift.\n",
    "<br>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise\t2 – The\tFP-growth algorithm\n",
    "\n",
    "#### a) Create the ordered initial F-list\n",
    "```\n",
    "F-list:\n",
    "{K} 1.0\n",
    "{E} 0.8\n",
    "{M} 0.6\n",
    "{O} 0.6\n",
    "{Y} 0.6\n",
    "{N} 0.4\n",
    "{C} 0.4\n",
    "{D} 0.2\n",
    "{A} 0.2\n",
    "{U} 0.2\n",
    "{I} 0.2\n",
    "\n",
    "{K, E, M, O, Y N, C, D, A, U, I}\n",
    "```\n",
    "#### b) Create the initial FP-tree"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "# display.Image(\"/Users/rale/Documents/Programming/csci349_2021sp/hw/img.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### c) Execute the FP_growth algorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# display.Image(\"/Users/rale/Documents/Programming/csci349_2021sp/hw/img2.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### d) Compare and contrast the computational requirements\n",
    "\n",
    "Apriori feels faster to do on paper, but the tree implimentation makes the space used much less than every single one of the aprioris which needs to be pruned. In the end the fpgrowth is a more modern approach with less speed and space saved. fpgrowth also only needs two scans versus the multiscan of the apriori. All of this was done to prevent the need to do subset itemset generation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise\t3 – The Eclat algorithm\n",
    "\n",
    "#### a) Convert the dataset in Exercise 1 to a vertical data format\n",
    "```\n",
    "{K} : [T100, T200, T300, T400, T500]\n",
    "{E} : [T100, T200, T300, T500]\n",
    "{O} : [T100, T200, T500]\n",
    "{M} : [T100, T300, T400]\n",
    "{Y} : [T100, T200, T400]\n",
    "{N} : [T100, T200]\n",
    "{C} : [T400, T500]\n",
    "{D} : [T200]\n",
    "{A} : [T300]\n",
    "{U} : [T400]\n",
    "{I} : [T500]\n",
    "```\n",
    "\n",
    "#### b) Find the frequent itemsets using the Eclat algorithm.\n",
    "```\n",
    "min_sup = 3\n",
    "k = 1\n",
    "{K}: [T100, T200, T300, T400, T500]\n",
    "{E}: [T100, T200, T300, T500]\n",
    "{O}: [T100, T200, T500]\n",
    "{M}: [T100, T300, T400]\n",
    "{Y}: [T100, T200, T400]\n",
    "\n",
    "k = 2\n",
    "{K, E}: [T100, T200, T300, T500]\n",
    "{K, O}: [T100, T200, T500]\n",
    "{K, M}: [T100, T300, T400]\n",
    "{K, Y}: [T100, T200, T400]\n",
    "{E, O}: [T100, T200, T500]\n",
    "{E, M}: [T100, T300]\n",
    "{E, Y}: [T100, T200]\n",
    "{O, M}: [T100]\n",
    "{O, Y}: [T100, T200]\n",
    "{M, Y}: [T100, T400]\n",
    "\n",
    "k = 3\n",
    "{K, E, O}: [T100, T200, T500]\n",
    "{K, E, M}: [T100, T300]\n",
    "{K, E, Y}: [T100, T200]\n",
    "{K, O, M}: [T100]\n",
    "{K, O, Y}: [T100, T200]\n",
    "{K, M, Y}: [T100, T400]\n",
    "{E, O, M}: [T100]\n",
    "{E, O, Y}: [T100, T200]\n",
    "{E, M, Y}: [T100]\n",
    "{O, M, Y}: [T100]\n",
    "\n",
    "k = 4\n",
    "{K, E, O, M}: [T100]\n",
    "{K, E, O, Y}: [T100, T200]\n",
    "{K, E, M, Y}: [T100]\n",
    "{K, O, M, Y}: [T100]\n",
    "{E, O, M, Y}: [T100]\n",
    "```\n",
    "```\n",
    "{K}: [T100, T200, T300, T400, T500]\n",
    "{E}: [T100, T200, T300, T500]\n",
    "{O}: [T100, T200, T500]\n",
    "{M}: [T100, T300, T400]\n",
    "{Y}: [T100, T200, T400]\n",
    "{K, E}: [T100, T200, T300, T500]\n",
    "{K, O}: [T100, T200, T500]\n",
    "{K, M}: [T100, T300, T400]\n",
    "{K, Y}: [T100, T200, T400]\n",
    "{E, O}: [T100, T200, T500]\n",
    "{K, E, O}: [T100, T200, T500]\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise\t4 – Correlation\n",
    "\n",
    "| . | A | ~A | total |\n",
    "| - | - | - |-|\n",
    "| B  | 65 | 40 | 105|\n",
    "| ~B | 35 | 10 | 45|\n",
    "| total | 100 | 50 | 150|\n",
    "\n",
    "#### a) Compute support and confidence for the rule A->B. Is this a strong rule?\n",
    "```\n",
    "min_sup = 0.4\n",
    "min_conf = 0.6\n",
    "\n",
    "support_AB = 65/150 = 43.3%\n",
    "support_A = 100/150 = 66.6%\n",
    "confidence = 0.433/0.666 = 65%\n",
    "\n",
    "```\n",
    "\n",
    "This is a strong rule, the confidence and support is high enough over the threshold, but not extremely strong since it is close to the threshold.\n",
    "\n",
    "\n",
    "#### b) What does the lift measure tell us? Compute lift(A,B). What does this suggest about the occurrence of A and B? What does it suggest about the rule?\n",
    "\n",
    "Lift tells us about the dependency between the two variables\n",
    "<br>\n",
    "```\n",
    "support_B = 105/150 = 70%\n",
    "lift = 0.433/(0.666*0.7) = 0.9287859288\n",
    "```\n",
    "This means that there is a significant correlation between A and B, but negatively, meaning that A prevents B instead of encouraging.\n",
    "\n",
    "#### c) Compute the expected values for each observed value above, showing your results in a table.\n",
    "\n",
    "| . | A | ~A |\n",
    "| - | - | -\n",
    "| B  | 70 | 35 |\n",
    "| ~B | 30 | 15 |\n",
    "\n",
    "\n",
    "#### d) Compute the X2 correlation coefficient using the table above and your expected values you computed in the previous question. Does the value imply dependency among A and B?\n",
    "\n",
    "```\n",
    "5*5/70 = 0.3571428571\n",
    "5*5/35 = 0.7142857143\n",
    "5*5/30 = 0.8333333333\n",
    "5*5/15 = 1.6666666667\n",
    "```\n",
    "\n",
    "$$\\chi^2 = 3.57$$\n",
    "\n",
    "With 1 degree of freedom, 3.57 lies between 0.10 and 0.5, which implies there is dependence but very slight.\n",
    "\n",
    "#### e) Consider the rule A -> NOT B. What is the support, confidence and lift for this rule?\n",
    "\n",
    "```\n",
    "support_A~B = 35/150 = 23.3%\n",
    "support_A = 100/150 = 66.6%\n",
    "confidence = 0.233/0.666 = 35%\n",
    "\n",
    "support_~B = 45/150 = 30%\n",
    "lift = 0.233/(0.666*0.3) = 1.166\n",
    "```\n",
    "#### f) What is the confidence and lift of the rule NOT B -> A ? You should notice there is an imbalance between your answer here and the previous question. Which rule is stronger? Why?\n",
    "\n",
    "\n",
    "```\n",
    "support_~BA = 35/150 = 23.3%\n",
    "support_~B = 45/150 = 30%\n",
    "confidence = 0.233/0.30 = 77%\n",
    "\n",
    "support_A = 100/150 = 66.6%\n",
    "lift = 0.233/(0.666*0.3) = 1.166\n",
    "```\n",
    "\n",
    "This rule is stronger, since the confidence is more than double. This is because the inverse is not the same, and this shows that ~B implies A moreso and not the other way around.\n",
    "#### g) Compute the Kulczynski measure for the items A and NOT B.\n",
    "\n",
    "$$K = \\frac{1}{2}\\big(P(A|\\neg B) + P(\\neg\tB|A)\\big)$$\n",
    "\n",
    "```\n",
    "A given ~B = 35/100 = 0.35\n",
    "~B given A = 35/45 = 0.78\n",
    "\n",
    "K = 0.564\n",
    "```\n",
    "\n",
    "#### h) Compute the imbalance ratio (IR) on A and NOT B. What do these results say? Does the result confirm your observations on questions e) and f) above?\n",
    "\n",
    "\n",
    "$$ IR = \\frac{\\big|support(A) - support(\\neg B)\\big|}{support(A) + support(\\neg B) - support(A \\cup \\neg B)}$$\n",
    "\n",
    "```\n",
    "support_A = 100/150 = 66.6%\n",
    "support_~B = 45/150 = 30%\n",
    "support_A~B = 35/150 = 23.3%\n",
    "\n",
    "IR = (0.66 - 0.3) / (0.66 + 0.3 - 0.233)\n",
    "   = 0.4951856946\n",
    "```\n",
    "\n",
    "These results show that there indeed in a coorlation between these two observations. The K is  close to 0.5 which means there is low dependency. The IR is not close to 0, which then we can take as being significant and shows a form of skewness."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise\t5 – Distributed\tmining\n",
    "#### Suppose that a large store has a transaction database that is distributed among four locations. Transactions in each component database have the same format, namely T_j: {i_1...i_m} where Tj is a transaction identifier, and i_k (1<=k<=m) is the identifier of an item purchased in the transaction. Propose an efficient algorithm to mine global association rules (without considering multilevel associations).\n",
    "\n",
    "Partition: Scan DB Only Twice, we can use the apriori principles to help in\n",
    "pruning candidates across the four dbs without the multilevel associations.\n",
    "\n",
    "\n",
    "Source: [Section V](http://www.ijesit.com/Volume%202/Issue%205/IJESIT201305_23.pdf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}